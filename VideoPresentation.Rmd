---
title: "Predicting Turbulence Simulations"
author: "Enzo Moraes Mescall, Martin Olarte, Charlotte Coudert"
date: "2022-11-05"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = FALSE)

# Report should be no more than 4 pages
```


```{r libraries, echo = FALSE, results = FALSE, message = FALSE}
library(tidyverse)
library(splines)
library(boot)
library(cowplot)
```

```{r data}
data.test = read.csv("data-test.csv")
data.train = read.csv("data-train.csv") %>%
  mutate(Fr = as.factor(Fr),
         Re = as.factor(Re))
```

## “The last great unsolved problem in classical physics”

![Turbulence has been described as “the last great unsolved problem in classical physics” by renowned physicist Richard Feynman](feynman.jpg)

## Navier Stokes

Direct numerical simulations (DNS) of complex differential equations can be very computationally expensive

$$\frac{\partial \rho}{\partial t} + \overrightarrow{\nabla}\cdot(\rho\overrightarrow{u})=0$$

$$\frac{\partial(\rho \overrightarrow{u})}{\partial t} + \overrightarrow{\nabla}\cdot[\rho\overline{\overline{u\otimes u}}] = -\overrightarrow{\nabla p} + \overrightarrow{\nabla}\cdot\overline{\overline{\tau}} + \rho\overrightarrow{f}$$
$$\frac{\partial(\rho e)}{\partial t} + \overrightarrow{\nabla}\cdot((\rho e + p)\overrightarrow{u}) = \overrightarrow{\nabla}\cdot(\overline{\overline{\tau}}\cdot\overrightarrow{u}) + \rho\overrightarrow{f}\overrightarrow{u} + \overrightarrow{\nabla}\cdot(\overrightarrow{\dot{q}})+r$$


## Initial EDA


```{r}
re_over = data.train %>%
  filter(Re == 90) %>%
  mutate(Fr = as.factor(Fr),
         Re = as.factor(Re))

re_under = data.train %>%
  filter(Re != 90) %>%
  mutate(Fr = as.factor(Fr),
         Re = as.factor(Re))

st_re_fr_1_plot <- ggplot(aes(x = St, y = R_moment_1, color = Re, shape = Fr), data = data.train) +
  labs(title = "St factored by Re and Fr vs. 1st Moment", x = "St", y = "1st Moment") + 
  theme(title = element_text(size = 6)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

st_re_fr_2_plot <- ggplot(aes(x = St, y = R_moment_2, color = Re, shape = Fr), data = data.train) +
  labs(title = "St factored by Re and Fr vs. 2nd Moment", x = "St", y = "2nd Moment") + 
  theme(title = element_text(size = 6)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

st_re_fr_3_plot <- ggplot(aes(x = St, y = R_moment_3, color = Re, shape = Fr), data = data.train) +
  labs(title = "St factored by Re and Fr vs. 3rd Moment", x = "St", y = "3rd Moment") + 
  theme(title = element_text(size = 6)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

st_re_fr_4_plot <- ggplot(aes(x = St, y = R_moment_4, color = Re, shape = Fr), data = data.train) +
  labs(title = "St factored by Re and Fr vs. 4th Moment", x = "St", y = "4th Moment") + 
  theme(title = element_text(size = 6)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

plot_grid(st_re_fr_1_plot, st_re_fr_2_plot, st_re_fr_3_plot, st_re_fr_4_plot)
```


## Initial Issues

Vast differences in orders of magnitude within the data

```{r}
st_re90_fr_2_plot <- ggplot(aes(x = St, y = R_moment_2, color = Re, shape = Fr), data = re_over) +
  labs(title = "St factored by Fr vs. 2nd Moment", subtitle = "For Re = 90", x = "St", y = "2nd Moment") + 
  theme(title = element_text(size = 6), plot.subtitle = element_text(size = 5)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

st_re90_plus_fr_2_plot <- ggplot(aes(x = St, y = R_moment_2, color = Re, shape = Fr), data = re_under) +
  labs(title = "St factored by Fr vs. 2nd Moment", subtitle = "For Re > 90", x = "St", y = "2nd Moment") + 
  theme(title = element_text(size = 6), plot.subtitle = element_text(size = 5)) +
  geom_smooth(method = "lm", formula = "y ~ log(x)") +
  geom_point()

plot_grid(st_re90_fr_2_plot, st_re90_plus_fr_2_plot)
```

## Initial model fitting approach

Simpler models tended to 'win'

```{r}
MAE = function(y_true, y_pred) mean(abs(y_true - y_pred))

errors = rep(NA, 10)
for (i in 2:10) {
  lm = glm(R_moment_1 ~ bs(St, df = i, degree = 2)*Re + Fr, data = data.train)
  errors[i] = suppressWarnings(cv.glm(data.train, lm, cost = MAE)$delta[1])
}

error_df <- data.frame(
  df = 2:10,
  errors = errors[-1]
)
error_df %>%
  ggplot(aes(x = df, y = errors)) +
  labs(title = "LOOCV MAE vs. Degrees of Freedom with Degree = 2 Splines", x = "DF", y = "Mean Absolute Error") +
  geom_line() +
  geom_point() +
  geom_vline(aes(xintercept = 2, col = "red")) +
  theme(legend.position = "none")
```

## The formula we landed on

All of the moments tended to have a similar structure

`Moment ~ log(St)*Fr*Re`

```{r}
re_over %>%
  filter(Fr == 0.052) %>%
  ggplot(aes(x = St, y = R_moment_2)) +
    labs(title = "St factored by Fr vs. 2nd Moment", subtitle = "For Re = 90, Fr = 0.052", x = "St", y = "2nd Moment") + 
    theme(title = element_text(size = 6), plot.subtitle = element_text(size = 5)) +
    geom_smooth(method = "lm", formula = "y ~ log(x)") +
    geom_point()
```


## Example: Model for the 4th raw moment

```{r echo = FALSE}
predictions = data.train

lm1 = glm(R_moment_1 ~ log(St)*Fr*Re + St*Fr*Re, data = data.train)
lm2 = glm(R_moment_2 ~ log(St)*Fr*Re + St*Fr*Re, data = data.train)

predictions$moment_1 = predict(lm1, predictions)

lm3 = glm(R_moment_3 ~ moment_1*Fr*Re, data = predictions)
lm4 = glm(R_moment_4 ~ log(St)*Fr*Re, data = data.train)

lm4
```

## Model diagnostics

```{r}
par(mfrow = c(2,2), mai = c(.4,.5,.5,.5))
plot(lm1)
```

## Model with logged response

```{r}
loglm1 = glm(log(R_moment_1) ~ log(St)*Fr*Re, data = data.train)

loglm1
```


## Model diagnostics with logged response

```{r}
par(mfrow = c(2,2), mai = c(.4,.5,.5,.5))
plot(loglm1)
```

## MSE difference

```{r}
all_errors_df <- data.frame(
  model = 1:4,
  error = c(6.25e-05, 359, 3.79e+10, 3.53e+18),
  log_error = c(7.84e-05, 7.61e+4, 8.25e+13, 3.73e+23)
)

all_errors_df %>%
  knitr::kable(col.names = c("Moment", "Regular MSE", "Log response MSE"), caption = "MSE difference between log and non-log models")
```

## Results

- Our final models had highly accurate predictions of the training set for moments 1 and 2 but got further from the true data into moments 3 and 4. 

- The model is more interpretable than a spline or other models may as the linearity allows us to look at coefficients for specific interactions and variables explicitly.

## Interpretations

- The main scientific finding is that, generally as can be see in our graphs, there is a logarithmic relationship between Stokes number and each moment with its coefficient dependent on its values of Froude number and Reynolds number. 

- All moments increase with St with the moments of Re = 90, the smallest Re, and Fr = 0.052, the smallest Fr, creating much largest moments of all degrees.

- This means that, comparing these levels of Re and Fr, a smaller fluid turbulence and gravitational acceleration are correlated with larger particle clusters. For all interactions, cluster size increases logarithmically with Stokes, which quantifies particle density and size.

## Limitations

- The most significant weakness in the project is that our final regression models all had rather poor model diagnostics. Analyzing the residuals reveals a multitude of outliers, non-constant variance, and a relationship that is unlikely to be normal. 

```{r fig.height=3, fig.width=6}
par(mfrow = c(2,2), mai = c(.4,.5,.5,.5))
plot(lm1)
```

## Limitations

- However, this can all be rapidly fixed by re-fitting the models with identical predictors but fitting them to the natural log of the response variables.

- The reason we decided to rely on the former is because the log fit models have significantly higher MSE than their regular counterparts, ranging from a 14% increase for the first moment to a multiple order of magnitude increase for the later moments.

## Continuation

- A possible extension to these models if we had more time would be to simulate more data. With this we could have more accurate prediction, a full model without NAs for the points we have currently nothing to train on, and enough to separate more training and testing data for fitting. 

- Additionally, with more time and beyond the scope of this course, we would be interested to train a neural network to improve prediction on the test set.